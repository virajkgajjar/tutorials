{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pyspark Handling Missing Values\n",
    "- Dropping Columns\n",
    "- Dropping Rows\n",
    "- Various Parameter In Dropping Functionalities\n",
    "- Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName('test').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+------+\n",
      "|  name| age| exp|salary|\n",
      "+------+----+----+------+\n",
      "|   bob|  23|   7| 63000|\n",
      "| saget|  26|   9| 75000|\n",
      "| james|  21|   5| 25000|\n",
      "|  paul|  29|   9|100000|\n",
      "| rohan|  32|  10| 50000|\n",
      "| jagat|  23|   2| 23000|\n",
      "|mahesh|NULL|NULL| 40000|\n",
      "|  NULL|  38|  12|110000|\n",
      "|  NULL|  36|NULL|  NULL|\n",
      "+------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark=spark.read.csv('test_data_tutorial3.csv', header=True, inferSchema=True)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+------+\n",
      "| name|age|exp|salary|\n",
      "+-----+---+---+------+\n",
      "|  bob| 23|  7| 63000|\n",
      "|saget| 26|  9| 75000|\n",
      "|james| 21|  5| 25000|\n",
      "| paul| 29|  9|100000|\n",
      "|rohan| 32| 10| 50000|\n",
      "|jagat| 23|  2| 23000|\n",
      "+-----+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop rows with null values in them\n",
    "df_pyspark.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+------+\n",
      "|  name| age| exp|salary|\n",
      "+------+----+----+------+\n",
      "|   bob|  23|   7| 63000|\n",
      "| saget|  26|   9| 75000|\n",
      "| james|  21|   5| 25000|\n",
      "|  paul|  29|   9|100000|\n",
      "| rohan|  32|  10| 50000|\n",
      "| jagat|  23|   2| 23000|\n",
      "|mahesh|NULL|NULL| 40000|\n",
      "|  NULL|  38|  12|110000|\n",
      "|  NULL|  36|NULL|  NULL|\n",
      "+------+----+----+------+\n",
      "\n",
      "+-----+---+---+------+\n",
      "| name|age|exp|salary|\n",
      "+-----+---+---+------+\n",
      "|  bob| 23|  7| 63000|\n",
      "|saget| 26|  9| 75000|\n",
      "|james| 21|  5| 25000|\n",
      "| paul| 29|  9|100000|\n",
      "|rohan| 32| 10| 50000|\n",
      "|jagat| 23|  2| 23000|\n",
      "+-----+---+---+------+\n",
      "\n",
      "+------+----+----+------+\n",
      "|  name| age| exp|salary|\n",
      "+------+----+----+------+\n",
      "|   bob|  23|   7| 63000|\n",
      "| saget|  26|   9| 75000|\n",
      "| james|  21|   5| 25000|\n",
      "|  paul|  29|   9|100000|\n",
      "| rohan|  32|  10| 50000|\n",
      "| jagat|  23|   2| 23000|\n",
      "|mahesh|NULL|NULL| 40000|\n",
      "|  NULL|  38|  12|110000|\n",
      "+------+----+----+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, age: int, exp: int, salary: int]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.na.drop(how=\"all\").show() #drop a row if all the values are null\n",
    "df_pyspark.na.drop(how=\"any\").show() #default: drop if any of the values in the row are null\n",
    "df_pyspark.na.drop(how=\"any\", thresh=2).show() # drop a row if it doesn't have atleast 'thresh' NON-null values\n",
    "df_pyspark.na.drop(how=\"any\", subset=['exp']) # drop entire row if the specified column has a NAN value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+----+------+\n",
      "|          name| age| exp|salary|\n",
      "+--------------+----+----+------+\n",
      "|           bob|  23|   7| 63000|\n",
      "|         saget|  26|   9| 75000|\n",
      "|         james|  21|   5| 25000|\n",
      "|          paul|  29|   9|100000|\n",
      "|         rohan|  32|  10| 50000|\n",
      "|         jagat|  23|   2| 23000|\n",
      "|        mahesh|NULL|NULL| 40000|\n",
      "|Missing Values|  38|  12|110000|\n",
      "|Missing Values|  36|NULL|  NULL|\n",
      "+--------------+----+----+------+\n",
      "\n",
      "+------+---+---+------+\n",
      "|  name|age|exp|salary|\n",
      "+------+---+---+------+\n",
      "|   bob| 23|  7| 63000|\n",
      "| saget| 26|  9| 75000|\n",
      "| james| 21|  5| 25000|\n",
      "|  paul| 29|  9|100000|\n",
      "| rohan| 32| 10| 50000|\n",
      "| jagat| 23|  2| 23000|\n",
      "|mahesh| 10| 10| 40000|\n",
      "|  NULL| 38| 12|110000|\n",
      "|  NULL| 36| 10|  NULL|\n",
      "+------+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filling the missing value\n",
    "df_pyspark.na.fill('Missing Values').show() # repalce NANs with specifed value (only if the data-type matches). For instance, this statement will only replace missing (NULL) names.\n",
    "df_pyspark.na.fill(10, ['age', 'exp']).show() # replace for specified columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+------+\n",
      "|  name| age| exp|salary|\n",
      "+------+----+----+------+\n",
      "|   bob|  23|   7| 63000|\n",
      "| saget|  26|   9| 75000|\n",
      "| james|  21|   5| 25000|\n",
      "|  paul|  29|   9|100000|\n",
      "| rohan|  32|  10| 50000|\n",
      "| jagat|  23|   2| 23000|\n",
      "|mahesh|NULL|NULL| 40000|\n",
      "|  NULL|  38|  12|110000|\n",
      "|  NULL|  36|NULL|  NULL|\n",
      "+------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the NULLs using imputer function\n",
    "\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=['age', 'exp', 'salary'],\n",
    "    outputCols=[\"{}_imputed\".format(c) for c in ['age', 'exp', 'salary']]\n",
    ").setStrategy(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+------+-----------+-----------+--------------+\n",
      "|  name| age| exp|salary|age_imputed|exp_imputed|salary_imputed|\n",
      "+------+----+----+------+-----------+-----------+--------------+\n",
      "|   bob|  23|   7| 63000|         23|          7|         63000|\n",
      "| saget|  26|   9| 75000|         26|          9|         75000|\n",
      "| james|  21|   5| 25000|         21|          5|         25000|\n",
      "|  paul|  29|   9|100000|         29|          9|        100000|\n",
      "| rohan|  32|  10| 50000|         32|         10|         50000|\n",
      "| jagat|  23|   2| 23000|         23|          2|         23000|\n",
      "|mahesh|NULL|NULL| 40000|         28|          7|         40000|\n",
      "|  NULL|  38|  12|110000|         38|         12|        110000|\n",
      "|  NULL|  36|NULL|  NULL|         36|          7|         60750|\n",
      "+------+----+----+------+-----------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
